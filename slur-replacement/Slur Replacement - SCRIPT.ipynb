{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import Levenshtein as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save slurs in a file\n",
    "\n",
    "slurs_list_lower = ['#rheachakraborty',\n",
    " '#498a',\n",
    " '#feministmafia',\n",
    " '#carryminatiroast',\n",
    " '#justiceforswapnilpandey',\n",
    " '#arrestranaayyub',\n",
    " '#boycottfabindia',\n",
    " '#domesticviolence',\n",
    " '#dowry',\n",
    " '#falsecases',\n",
    " '#feministsaretrash',\n",
    " '#genderbiasedlaws',\n",
    " '#hiteshachandranee',\n",
    " '#love_nahi_jeehad_hai',\n",
    " '#justiceforcabdriver',\n",
    " '#menscommission',\n",
    " '#menslivesmatter',\n",
    " '#potentiallegalterrorist',\n",
    " '#mentoo',\n",
    " '#womenaretrash',\n",
    " 'presstitutes',\n",
    " 'liberal',\n",
    " '#‡ÆÖ‡Æ§‡Øç‡Æ§‡Øà',\n",
    " '#speakupmen',\n",
    " '#ranaayyub',\n",
    " '#sullideals',\n",
    " '#pussypower',\n",
    " '#reservation_kills_merit',\n",
    " '#reservation',\n",
    " '#superstraight',\n",
    " '#randikutiya',\n",
    " '#save_hindu_girls',\n",
    " '‡§ú‡§ø‡§π‡§æ‡§¶‡•Ä',\n",
    " 'pseudohindus',\n",
    " 'chrislamocommies',\n",
    " 'sanghans',\n",
    " 'nationalists',\n",
    " 'whore',\n",
    " 'faggot',\n",
    " 'love jihad',\n",
    " '‡§∞‡§Ç‡§°‡•Ä',\n",
    " '‡§∞‡§£‡•ç‡§°‡•Ä',\n",
    " 'libtard',\n",
    " '‡§∞‡§Ç‡§°‡•Ä‡§ñ‡§æ‡§®‡§æ',\n",
    " 'deshdrohi chinese',\n",
    " '‡§∞‡§£‡•ç‡§°‡•Ä ‡§∞‡•ã‡§®‡§æ',\n",
    " '‡§≤‡•Å‡§≤‡•ç‡§≤‡•Ä',\n",
    " '‡§ó‡§æ‡§Ç‡§°',\n",
    " '‡§ï‡•Å‡§§‡§ø‡§Ø‡§æ',\n",
    " '‡§ï‡•Å‡§§‡•ç‡§§‡•Ä',\n",
    " '‡§ï‡§Æ‡•Ä‡§®‡•Ä',\n",
    " '‡ÆÖ‡Æ§‡Øç‡Æ§‡Øà',\n",
    " '‡Æü‡Æø‡Æï‡Æø',\n",
    " '‡ÆÖ‡Æ∞‡Æµ‡Ææ‡Æ£‡Æø',\n",
    " '‡ÆÖ‡Æ≤‡Æø',\n",
    " '‡Æï‡Æ≤‡Øç‡Æ≤‡ØÅ',\n",
    " '‡Æ™‡Øä‡Æ©‡Øç‡Æ∏‡Øç',\n",
    " '‡Æí‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ',\n",
    " '‡§Æ‡•Å‡§ú‡§∞‡§æ',\n",
    " '‡§π‡§ø‡§ú‡•ú‡§æ',\n",
    " '‡§¨‡§¶‡§∏‡•Ç‡§∞‡§§',\n",
    " '‡§¨‡§≤‡§æ‡§§‡•ç‡§ï‡§æ‡§∞',\n",
    " '‡§≤‡§ø‡§¨‡•ç‡§∞‡§æ‡§£‡•ç‡§°‡•Ç',\n",
    " '‡§®‡§Ç‡§ó‡•Ä_‡§™‡•Å‡§Ç‡§ó‡•Ä',\n",
    " '‡§™‡§æ‡§ó‡§≤ ‡§î‡§∞‡§§',\n",
    " 'rape',\n",
    " 'r@pe',\n",
    " 'r@p3',\n",
    " 'bitch',\n",
    " 'victim_card',\n",
    " 'sekoolar',\n",
    " 'sickular',\n",
    " 'sc0undrel',\n",
    " 'r@ndi',\n",
    " 'feminazi',\n",
    " 'chinese corona',\n",
    " '‡Æ™‡Øä‡ÆÆ‡Øç‡Æ™‡Æ≥ ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø',\n",
    " '‡Æô‡Øç‡Æï‡Øã‡Æ§‡Øç‡Æ§‡Ææ',\n",
    " '‡Æï‡Øã‡Æ§‡Øç‡Æ§‡Ææ',\n",
    " '‡Æ§‡Ææ‡ÆØ‡Øã‡Æ≥‡Æø',\n",
    " '‡Æ§‡Øá‡Æµ‡Øç‡Æü‡Æø‡ÆØ‡Ææ ‡Æ™‡Øà‡ÆØ‡Ææ',\n",
    " '‡Æ§‡Øá‡Æµ‡Øç‡Æü‡Æø‡ÆØ‡Ææ',\n",
    " '‡Æö‡ØÇ‡Æ§‡Øç‡Æ§‡ØÅ',\n",
    " '‡Æ≤‡ØÜ‡Æ∏‡Øç‡Æ™‡Æø‡ÆØ‡Æ©‡Øç',\n",
    " '‡Æä‡ÆÆ‡Øç‡Æ™‡ØÅ',\n",
    " '‡Æ™‡ØÅ‡Æ£‡Øç‡Æü',\n",
    " '‡Æê‡Æü‡Øç‡Æü‡ÆÆ‡Øç',\n",
    " '‡ÆÖ‡ÆØ‡Æø‡Æü‡Øç‡Æü‡ÆÆ‡Øç',\n",
    " '‡Æö‡Ææ‡ÆÆ‡Ææ‡Æ©‡Øç',\n",
    " '‡Æï‡ØÇ‡Æ§‡Æø',\n",
    " '‡ÆÜ‡Æü‡Øç‡Æü‡Æï‡Øç‡Æï‡Ææ‡Æ∞‡Æø',\n",
    " '‡Æµ‡Øá‡Æö‡Øà',\n",
    " '‡Æµ‡Øá‡Æö',\n",
    " '‡Æ™‡Øä‡Æ§‡ØÅ‡Æö‡Øç ‡Æö‡Øä‡Æ§‡Øç‡Æ§‡ØÅ',\n",
    " '‡Æä‡Æ∞‡Øç ‡ÆÆ‡Øá‡ÆØ‡Øç‡Æ±‡Æ§‡ØÅ',\n",
    " '‡Æ™‡Øä‡Æü‡Øç‡Æü‡Øà',\n",
    " '‡ÆÆ‡Ææ‡Æô‡Øç‡Æï‡Ææ',\n",
    " '‡Æï‡Øã‡Æ¥‡Æø',\n",
    " '‡ÆÆ‡ØÅ‡Æ≤‡Øà',\n",
    " '‡Æ™‡Æ±‡Æ§‡Øç‡Æ§‡Ææ‡ÆØ‡Øã‡Æ≤‡Æø',\n",
    " '‡Æì‡Æï‡Øç‡Æï',\n",
    " '‡Æ§‡Øá‡Æµ‡Æü‡Æø‡ÆØ‡Ææ_‡ÆÆ‡Æµ‡Æ©‡Øç',\n",
    " '‡Æ§‡Øá‡Æµ‡Æü‡Æø‡ÆØ‡Ææ ‡Æ™‡Æö‡Æô‡Øç‡Æï‡Æ≥‡Ææ',\n",
    " '‡Æ™‡ØÅ‡Æ£‡Øç‡Æü‡Øà',\n",
    " 'atrocity_act',\n",
    " 'arrestlucknowgirl',\n",
    " 'fakecasewalibiwi',\n",
    " 'gasti',\n",
    " '#lovejihad',\n",
    " '#misandry',\n",
    " '#piddilover',\n",
    " '#pseudohindus',\n",
    " '#rheality_today',\n",
    " '#sanghans',\n",
    " '#saveoursons',\n",
    " '#scstact',\n",
    " 'üçë',\n",
    " '#‡§ú‡§ø‡§π‡§æ‡§¶‡•Ä_‡§Æ‡•Å‡§ï‡•ç‡§§_‡§®‡§µ‡§∞‡§æ‡§§‡•ç‡§∞‡§ø',\n",
    " '#‡Æì‡Æ§‡Øç‡Æ§',\n",
    " '‡§Ü‡§§‡§Ç‡§ï‡§µ‡§æ‡§¶‡•Ä',\n",
    " '‡§ï‡§ö‡§∞‡§æ',\n",
    " '‡§ï‡§Ç‡§ú‡§∞',\n",
    " '‡§ï‡§ü‡§æ ‡§≤‡•Å‡§Ç‡§°',\n",
    " '‡§ï‡§≤‡§Æ‡•Å‡§Ç‡§π‡•Ä',\n",
    " '‡§ï‡§∏‡§æ‡§à',\n",
    " '‡§ï‡§æ‡§Æ_‡§µ‡§æ‡§≤‡•Ä_‡§¨‡§æ‡§à',\n",
    " '‡§ï‡§æ‡§≤‡§æ ‡§ü‡•á‡§Ç‡§ü',\n",
    " '‡§ï‡§æ‡§≤‡•Ä ‡§ï‡§≤‡•Ç‡§ü‡•Ä',\n",
    " '‡§ï‡§æ‡§≤‡•á ‡§§‡§µ‡•á',\n",
    " '‡§ï‡§ø‡§®‡•ç‡§®‡§∞',\n",
    " '‡§ï‡•Å‡§≤‡•ç‡§ü‡§æ',\n",
    " '‡•ô‡•Å‡§∏‡§∞‡§æ',\n",
    " '‡§ó‡§ü‡§∞ ‡§™‡•à‡§¶‡§æ‡§á‡§∂',\n",
    " '‡§ó‡§£‡§ø‡§ï‡§æ',\n",
    " '‡§ó‡§¶‡•ç‡§¶‡§æ‡§∞',\n",
    " '‡§ó‡§ß‡•Ä',\n",
    " '‡§ò‡§ü‡§ø‡§Ø‡§æ ‡§î‡§∞‡§§',\n",
    " '‡§ö‡§Ç‡§°‡§æ‡§≤',\n",
    " '‡§ö‡§Ç‡§°‡§æ‡§≤_‡§ö‡•å‡§ï‡§°‡§º‡•Ä',\n",
    " '‡§ö‡§Æ‡§ö‡§æ',\n",
    " '‡§ö‡§Æ‡§æ‡§∞',\n",
    " '‡§ö‡§∞‡§ø‡§§‡•ç‡§∞‡§π‡•Ä‡§®',\n",
    " '‡§ö‡§æ‡§ü‡•Å‡§ï‡§æ‡§∞',\n",
    " '‡§ö‡§æ‡§≤‡•Ç ‡§î‡§∞‡§§',\n",
    " '‡§ö‡•Å‡§¶‡§æ‡§à',\n",
    " '‡§ö‡•Å‡§∏‡§≤‡•ç‡§Æ‡§æ‡§®',\n",
    " '‡§ö‡•Å‡§∏‡•ç‡§≤‡§æ‡§Æ‡§ø',\n",
    " '‡§ö‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ',\n",
    " '‡§ö‡•Ç‡§ö‡•Ä',\n",
    " '‡§ö‡•Ç‡•ú‡§æ',\n",
    " '‡§ö‡•Ç‡§∏',\n",
    " '‡§õ‡§ø‡§®‡§æ‡§≤',\n",
    " '‡§ú‡§Ç‡§ó‡§≤‡•Ä',\n",
    " '‡§ú‡§≤‡•Ä',\n",
    " '‡§ú‡§æ‡§π‡§ø‡§≤_‡§î‡§∞‡§§',\n",
    " '‡§ú‡•Ç‡§§‡§æ ‡§ñ‡§æ‡§Ø‡•á‡§ó‡•Ä',\n",
    " '‡§ú‡•ã‡§∞‡•Ç ‡§ï‡§æ ‡§ó‡•Å‡§≤‡§æ‡§Æ',\n",
    " '‡§ù‡•Ç‡§†‡•Ä ‡§î‡§∞‡§§',\n",
    " '‡§õ‡•Å‡§§‡•Ä‡§Ø‡•á',\n",
    " '‡§§‡§µ‡§æ‡§á‡•û',\n",
    " '‡§¶‡§≤‡§æ‡§≤',\n",
    " '‡§¶‡•á‡§π‡§æ‡§§‡§®',\n",
    " '‡§¶‡•á‡§π‡§æ‡§§‡•Ä ‡§î‡§∞‡§§',\n",
    " '‡§¶‡•ã ‡§ï‡•å‡•ú‡•Ä ‡§ï‡•Ä ‡§î‡§∞‡§§',\n",
    " '‡§¶‡•ã‡§ó‡§≤‡•Ä',\n",
    " '‡§ß‡•ã‡§¨‡•Ä',\n",
    " '‡§®‡§≤‡•ç‡§≤‡•Ä',\n",
    " '‡§®‡§æ‡§ú‡§æ‡§Ø‡•õ',\n",
    " '‡§®‡§æ‡§≤',\n",
    " '‡§™‡§®‡•å‡§§‡•Ä',\n",
    " '‡§™‡§ø‡§õ‡§µ‡§æ‡•ú‡§æ',\n",
    " '‡§™‡•á‡§≤',\n",
    " '‡§™‡•à‡§∞ ‡§ï‡•Ä ‡§ú‡•Ç‡§§‡•Ä',\n",
    " '‡§™‡•â‡§ü‡•Ä',\n",
    " '‡§´‡•á‡§Æ‡§ø‡§®‡§ø‡§ú‡§Æ',\n",
    " '‡§¨‡§§‡•ç‡§§‡§Æ‡•Ä‡•õ',\n",
    " '‡§¨‡§π‡§® ‡§ï‡•Ä ‡§≤‡•ã‡•ú‡•Ä',\n",
    " '‡§¨‡§π‡§® ‡§ö‡•ã‡§¶',\n",
    " '‡§¨‡§π‡§®‡§ú‡•Ä',\n",
    " '‡§¨‡§æ‡•õ‡§æ‡§∞‡•Ç ‡§î‡§∞‡§§',\n",
    " '‡§¨‡•Ä‡§¨‡•Ä',\n",
    " '‡§¨‡•Å‡§∞‡§ñ‡§æ ‡§ß‡§§‡•ç‡§§',\n",
    " '‡§¨‡•Å‡§∞‡§ñ‡•á ‡§µ‡§æ‡§≤‡•Ä',\n",
    " '‡§¨‡•Å‡§≤‡•ç‡§≤‡•Ä',\n",
    " '‡§¨‡•á‡§π‡•Ç‡§¶‡§æ ‡§î‡§∞‡§§',\n",
    " '‡§¨‡•à‡§≤ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø',\n",
    " '‡§≠‡§Ç‡§ó‡•Ä',\n",
    " '‡§≠‡•ú‡§µ‡§æ',\n",
    " '‡§≠‡§¶‡•ç‡§¶‡•Ä ‡§î‡§∞‡§§',\n",
    " '‡§≠‡§æ‡§Ç‡§°',\n",
    " '‡§≠‡§æ‡§Ç‡§° ‡§î‡§∞‡§§',\n",
    " '‡§≠‡§æ‡•ú‡•á ‡§ï‡§æ ‡§ü‡§ü‡•ç‡§ü‡•Ç',\n",
    " '‡§≠‡§æ‡§∞‡§§ ‡§§‡•á‡§∞‡•á ‡§ü‡•Å‡§ï‡§°‡§º‡•á ‡§ó‡•à‡§Ç‡§ó',\n",
    " '‡§≠‡•Ç‡§§‡§®‡•Ä ‡§ï‡•á',\n",
    " '‡§≠‡•ã‡§Ç‡§ï‡§®‡§æ',\n",
    " '‡§≠‡•ã‡§∏‡§°‡§º‡•Äwala',\n",
    " '‡§Æ‡§∞ ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§ú‡§æ‡§§‡•Ä',\n",
    " '‡§Æ‡§π‡§æ‡§∞',\n",
    " '‡§Æ‡§æ ‡§ï‡•á ‡§≠‡•ã‡§∏‡§°‡§º‡•á',\n",
    " '‡§Æ‡§æ‡§¶‡§∞‡§ú‡§æ‡§§',\n",
    " '‡§Æ‡•Å‡§≤‡§ø‡§Ø‡§æ',\n",
    " '‡§Æ‡•Å‡§Å‡§π ‡§Æ‡•à‡§Ç ‡§≤‡•á',\n",
    " '‡§Æ‡•Ç‡§§',\n",
    " '‡§Æ‡•á‡§°‡§Æ ‡§ú‡•Ä',\n",
    " '‡§Æ‡•ã‡§ü‡•Ä ‡§≠‡•à‡§Ç‡§∏',\n",
    " '‡§Æ‡•ç‡§≤‡•á‡§ö‡•ç‡§õ‡§æ',\n",
    " '‡§∞‡§æ‡§Ç‡§°',\n",
    " '‡§≤‡§æ‡§®‡§§‡•Ä',\n",
    " '‡§≤‡•á‡§∏‡•ç‡§¨‡§ø‡§Ø‡§®',\n",
    " '‡§≤‡•ã‡•ú‡•Ç',\n",
    " '‡§≤‡•å‡•ú‡§æ',\n",
    " '‡§≤‡•å‡•ú‡•á',\n",
    " '‡§µ‡§ø‡§ï‡•ç‡§ü‡§ø‡§Æ ‡§ï‡§æ‡§∞‡•ç‡§°',\n",
    " '‡§∂‡§∞‡§ø‡§Ø‡§æ',\n",
    " '‡§∏‡§æ‡§≤‡•Ä',\n",
    " '‡§∏‡•Å‡§≤‡•ç‡§≤‡•Ä',\n",
    " '‡§∏‡•Ç‡§Ö‡§∞',\n",
    " '‡§∏‡•Ç‡§Ö‡§∞ ‡§ï‡•Ä ‡§î‡§≤‡§æ‡§¶',\n",
    " '‡§∏‡•å ‡§ü‡§ï‡§æ ‡§ü‡§Ç‡§ö ‡§Æ‡§æ‡§≤',\n",
    " '‡§∏‡•ç‡§ï‡§∞‡•ç‡§ü ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§à',\n",
    " '‡§π‡§ó‡§®‡§æ',\n",
    " '‡§π‡§¨‡§∂‡•Ä',\n",
    " '‡§π‡§∞‡§æ‡§Æ‡§ú‡§æ‡§¶‡•Ä',\n",
    " '‡§π‡§∞‡§æ‡§Æ‡•Ä',\n",
    " '‡§π‡§≤‡§æ‡§≤‡§æ',\n",
    " '‡§π‡§ø‡§ú‡§∞‡§æ',\n",
    " '‡§π‡§ø‡§ú‡§º‡§∞‡§æ‡§™‡§Ç‡§§‡•Ä',\n",
    " '‡§π‡§ø‡§≤‡§æ‡§ì‡§ó‡•Ä',\n",
    " '‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶',\n",
    " '‡§≠‡•ã‡§∏‡•ç‡§°‡•Ä‡§ï‡•á',\n",
    " '‡§ü‡§ü‡•ç‡§ü‡•Ä',\n",
    " '‡§ú‡§æ ‡§®‡§æ‡§≤‡•Ä ‡§∏‡§æ‡§´‡§º ‡§ï‡§∞‡§ï‡•á ‡§Ü',\n",
    " '‡§Ü‡§Ç‡§ü‡•Ä',\n",
    " '‡Æ≤‡ØÇ‡Æö‡ØÅ ‡Æï‡ØÇ',\n",
    " '‡Æ™‡Øä‡Æü‡Øç‡Æü‡Øà ‡Æ®‡Ææ‡ÆØ‡Øç',\n",
    " '‡Æ≤‡ØÇ‡Æö‡ØÅ',\n",
    " '‡Æ§‡Øá‡Æµ‡Æø‡Æü‡Æø‡ÆØ‡Ææ‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Øä‡Æ±‡Æ®‡Øç‡Æ§‡Æµ‡Æ©‡Øç',\n",
    " '‡Æ§‡Øá‡Æµ‡Æø‡Æü‡Æø‡ÆØ‡Ææ ‡Æ™‡ØÅ‡Æ£‡Øç‡Æü',\n",
    " '‡Æö‡ØÇ‡Æ§‡Øç‡Æ§‡Æü‡Æø',\n",
    " '‡Æï‡ØÇfire',\n",
    " '#‡§õ‡§ï‡•ç‡§ï‡§æ',\n",
    " '#‡Æï‡Øä‡ÆÆ‡Øç‡ÆÆ‡Ææ',\n",
    " '#‡Æô‡Øç‡Æï‡Øã‡Æ§‡Øç‡Æ§‡Ææ',\n",
    " '#‡Æö‡Ææ‡ÆÆ‡Ææ‡Æ©‡Øç',\n",
    " '#‡Æ™‡ØÜ‡Æ∞‡Æø‡ÆØ‡Ææ‡Æ∞‡Ææ‡Æµ‡Æ§‡ØÅ_‡ÆÆ‡ÆØ‡Æø‡Æ∞‡Ææ‡Æµ‡Æ§‡ØÅ',\n",
    " '#‡Æ™‡Øä‡Æü‡Øç‡Æü‡Øà',\n",
    " '#‡ÆÆ‡ØÅ‡Æü‡Øç‡Æü‡Ææ‡Æâ‡Æ™‡Æø',\n",
    " 'anti national tukde',\n",
    " '498a',\n",
    " 'bimbo',\n",
    " 'toxic aunty',\n",
    " 'toilet saaf',\n",
    " 'buzlim',\n",
    " 'characterless woman',\n",
    " 'chinky',\n",
    " 'black pepper',\n",
    " 'dull-it',\n",
    " 'dynast sycophants',\n",
    " 'ferrorists',\n",
    " 'izzlam',\n",
    " 'katwa',\n",
    " 'muzlim',\n",
    " 'naachne waali',\n",
    " 'ola u uber',\n",
    " 'pak agents',\n",
    " 'pheminist',\n",
    " 'pissfull',\n",
    " 'rice bags',\n",
    " 'scumbags',\n",
    " 'secular',\n",
    " 'sissy',\n",
    " 'dented-painted',\n",
    " 'toilet + saaf',\n",
    " 'sunflower‡Æ£‡Øç‡Æü‡Øà_emoji',\n",
    " 'sunflower‡Æ£‡Øç‡Æü‡Øà',\n",
    " 'scumreds',\n",
    " 'samlaingik',\n",
    " 'ma ki chui',\n",
    " 'jersey cow',\n",
    " 'burnol',\n",
    " 'victim card']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_exact_slurs(tweet,slurs_list_lower):\n",
    "    \n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    matches = re.findall(r\"(?=(\"+'|'.join(slurs_list_lower)+r\"))\", tweet.lower())\n",
    "    \n",
    "    slurs = []\n",
    "    tokens = []\n",
    "    print(matches)\n",
    "    \n",
    "    for match in matches:\n",
    "    \n",
    "        print(match)\n",
    "        \n",
    "        slurs.append(match)\n",
    "        \n",
    "        tokens.append(match)\n",
    "        \n",
    "        tweet = tweet.replace(match,'----')\n",
    "    \n",
    "        # token,slur key pair\n",
    "        result.update({match:match})\n",
    "    \n",
    "    return tweet,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_matching_slurs(tweet,slurs_list_lower,threshold_score=70):\n",
    "    \n",
    "    print(\"\\nApprox matching\")\n",
    "    check = 0\n",
    "         \n",
    "    match_dict = dict(process.extract(tweet,slurs_list_lower,limit = 10,scorer=fuzz.partial_ratio))\n",
    "\n",
    "    matches = match_dict.keys()\n",
    "    \n",
    "    # to compare the distance of top 10 matching slurs to find the right matching\n",
    "    dis_dict = {}\n",
    "    token_slur_dict = {}\n",
    "    \n",
    "    \n",
    "    for slur in matches:\n",
    "        \n",
    "        for token in tweet.split(' '):\n",
    "            \n",
    "            \"\"\"\n",
    "            Can add memoization here\n",
    "            \n",
    "            -Check if the distance b/w token and match is already calculated\n",
    "            \"\"\"\n",
    "            \n",
    "            if (token,slur) not in token_slur_dict:\n",
    "                \n",
    "                dis = lev.distance(token,slur)\n",
    "                            \n",
    "                token_slur_dict[(token,slur)] = dis\n",
    "            \n",
    "                if dis in dis_dict:\n",
    "                    dis_dict[dis].append((token,slur))\n",
    "            \n",
    "                else:\n",
    "                    dis_dict[dis] = [(token,slur)]\n",
    "            \n",
    "    \n",
    "    dist_sort = dict(sorted(dis_dict.items()))\n",
    "   \n",
    "    result_dict = {}\n",
    "    result = {}\n",
    "    \n",
    "    for dist,match in dist_sort.items():\n",
    "        \n",
    "        #print(dist,match)\n",
    "        loop_break = 0\n",
    "        \n",
    "        for token,slur in match:\n",
    "            \n",
    "            #does it work for hin and tamil?\n",
    "            if token:\n",
    "                \n",
    "                print(slur)\n",
    "                print(match_dict[slur])\n",
    "    \n",
    "                if (slur[0].lower() == token[0].lower()) and (match_dict[slur] >= threshold_score) and (token.lower() not in ['muslim','muslims']):\n",
    "                    \n",
    "                    print(f'slur,token : {slur} {token}')\n",
    "                    result_dict[('slur','token')] = (slur,token)\n",
    "                    \n",
    "                    #token,slur value pair\n",
    "                    result.update({token:slur})\n",
    "                    \n",
    "                  \n",
    "                    tweet = tweet.replace(token,'----')\n",
    "                    \n",
    "                    # to iterate all the matches in that dict\n",
    "                    loop_break = 1\n",
    "                    \n",
    "        if loop_break:\n",
    "            break\n",
    "                \n",
    "    return tweet,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slur_replacement_slurs_v1(tweet,slurs_list_lower,threshold_score=70):\n",
    "    \n",
    "    tweet1,exact_result = regex_exact_slurs(tweet,slurs_list_lower)\n",
    "    print(f'exact : {exact_result}')\n",
    "    tweet2,approx_result = approx_matching_slurs(tweet1,slurs_list_lower,threshold_score=threshold_score)\n",
    "    print(f'approx : {approx_result}')\n",
    "    \n",
    "    exact_result.update(approx_result)\n",
    "    print(f'combined : {exact_result}')\n",
    "    \n",
    "    return exact_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet = '‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£‡§µ‡§æ‡§¶‡•Ä ‡§™‡§ø‡§§‡•É‡§∏‡§§‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à? ‡§Ø‡•á ‡§¶‡•ã ‡§∂‡§¨‡•ç‡§¶ ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§∏‡•á ‡§ú‡•Å‡§°‡§º‡•á ‡§π‡•Å‡§è ‡§π‡•à. ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£‡§µ‡§æ‡§¶ ‡§Æ‡§æ‡§®‡•á ‡§ú‡•ã ‡§∏‡§Æ‡§æ‡§ú ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§§‡§ø‡§µ‡§æ‡§¶ ‡§ï‡•ã ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§™‡§ø‡§§‡•É‡§∏‡§§‡•ç‡§§‡§æ, ‡§Æ‡§æ‡§®‡•á ‡§ú‡•ã ‡§Æ‡§π‡§ø‡§≤‡§æ‡§ì‡§Ç ‡§™‡§∞ ‡§Ö‡§™‡§®‡§æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ú‡§Æ‡§æ‡§è ‡§∞‡§ñ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•à. ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£‡§µ‡§æ‡§¶‡•Ä ‡§™‡§ø‡§§‡•É‡§∏‡§§‡•ç‡§§‡§æ ‡§ú‡•ã ‡§ú‡§æ‡§§‡§ø ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ ‡§î‡§∞ ‡§™‡§ø‡§§‡•É‡§∏‡§§‡•ç‡§§‡§æ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§®‡§æ‡§Ø‡•á ‡§∞‡§ñ‡§§‡•Ä ‡§π‡•à.'\n",
    "slur_replacement_slurs_v1(tweet,slurs_list_lower,threshold_score=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "exact : {}\n",
      "\n",
      "Approx matching\n",
      "‡§≠‡•ã‡§Ç‡§ï‡§®‡§æ\n",
      "100\n",
      "slur,token : ‡§≠‡•ã‡§Ç‡§ï‡§®‡§æ ‡§≠‡•å‡§Ç‡§ï‡§®‡§æ\n",
      "approx : {'‡§≠‡•å‡§Ç‡§ï‡§®‡§æ': '‡§≠‡•ã‡§Ç‡§ï‡§®‡§æ'}\n",
      "combined : {'‡§≠‡•å‡§Ç‡§ï‡§®‡§æ': '‡§≠‡•ã‡§Ç‡§ï‡§®‡§æ'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'‡§≠‡•å‡§Ç‡§ï‡§®‡§æ': '‡§≠‡•ã‡§Ç‡§ï‡§®‡§æ'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slur_replacement_slurs_v1(tweet,slurs_list_lower,threshold_score=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
