import re
from fuzzywuzzy import process
from fuzzywuzzy import fuzz

import Levenshtein as lev

# save slurs in a file

slurs_list_lower = ['#rheachakraborty',
 '#498a',
 '#feministmafia',
 '#carryminatiroast',
 '#justiceforswapnilpandey',
 '#arrestranaayyub',
 '#boycottfabindia',
 '#domesticviolence',
 '#dowry',
 '#falsecases',
 '#feministsaretrash',
 '#genderbiasedlaws',
 '#hiteshachandranee',
 '#love_nahi_jeehad_hai',
 '#justiceforcabdriver',
 '#menscommission',
 '#menslivesmatter',
 '#potentiallegalterrorist',
 '#mentoo',
 '#womenaretrash',
 'presstitutes',
 'liberal',
 '#à®…à®¤à¯à®¤à¯ˆ',
 '#speakupmen',
 '#ranaayyub',
 '#sullideals',
 '#pussypower',
 '#reservation_kills_merit',
 '#reservation',
 '#superstraight',
 '#randikutiya',
 '#save_hindu_girls',
 'à¤œà¤¿à¤¹à¤¾à¤¦à¥€',
 'pseudohindus',
 'chrislamocommies',
 'sanghans',
 'nationalists',
 'whore',
 'faggot',
 'love jihad',
 'à¤°à¤‚à¤¡à¥€',
 'à¤°à¤£à¥à¤¡à¥€',
 'libtard',
 'à¤°à¤‚à¤¡à¥€à¤–à¤¾à¤¨à¤¾',
 'deshdrohi chinese',
 'à¤°à¤£à¥à¤¡à¥€ à¤°à¥‹à¤¨à¤¾',
 'à¤²à¥à¤²à¥à¤²à¥€',
 'à¤—à¤¾à¤‚à¤¡',
 'à¤•à¥à¤¤à¤¿à¤¯à¤¾',
 'à¤•à¥à¤¤à¥à¤¤à¥€',
 'à¤•à¤®à¥€à¤¨à¥€',
 'à®…à®¤à¯à®¤à¯ˆ',
 'à®Ÿà®¿à®•à®¿',
 'à®…à®°à®µà®¾à®£à®¿',
 'à®…à®²à®¿',
 'à®•à®²à¯à®²à¯',
 'à®ªà¯Šà®©à¯à®¸à¯',
 'à®’à®©à¯à®ªà®¤à¯',
 'à¤®à¥à¤œà¤°à¤¾',
 'à¤¹à¤¿à¤œà¥œà¤¾',
 'à¤¬à¤¦à¤¸à¥‚à¤°à¤¤',
 'à¤¬à¤²à¤¾à¤¤à¥à¤•à¤¾à¤°',
 'à¤²à¤¿à¤¬à¥à¤°à¤¾à¤£à¥à¤¡à¥‚',
 'à¤¨à¤‚à¤—à¥€_à¤ªà¥à¤‚à¤—à¥€',
 'à¤ªà¤¾à¤—à¤² à¤”à¤°à¤¤',
 'rape',
 'r@pe',
 'r@p3',
 'bitch',
 'victim_card',
 'sekoolar',
 'sickular',
 'sc0undrel',
 'r@ndi',
 'feminazi',
 'chinese corona',
 'à®ªà¯Šà®®à¯à®ªà®³ à®ªà¯Šà®°à¯à®•à¯à®•à®¿',
 'à®™à¯à®•à¯‹à®¤à¯à®¤à®¾',
 'à®•à¯‹à®¤à¯à®¤à®¾',
 'à®¤à®¾à®¯à¯‹à®³à®¿',
 'à®¤à¯‡à®µà¯à®Ÿà®¿à®¯à®¾ à®ªà¯ˆà®¯à®¾',
 'à®¤à¯‡à®µà¯à®Ÿà®¿à®¯à®¾',
 'à®šà¯‚à®¤à¯à®¤à¯',
 'à®²à¯†à®¸à¯à®ªà®¿à®¯à®©à¯',
 'à®Šà®®à¯à®ªà¯',
 'à®ªà¯à®£à¯à®Ÿ',
 'à®à®Ÿà¯à®Ÿà®®à¯',
 'à®…à®¯à®¿à®Ÿà¯à®Ÿà®®à¯',
 'à®šà®¾à®®à®¾à®©à¯',
 'à®•à¯‚à®¤à®¿',
 'à®†à®Ÿà¯à®Ÿà®•à¯à®•à®¾à®°à®¿',
 'à®µà¯‡à®šà¯ˆ',
 'à®µà¯‡à®š',
 'à®ªà¯Šà®¤à¯à®šà¯ à®šà¯Šà®¤à¯à®¤à¯',
 'à®Šà®°à¯ à®®à¯‡à®¯à¯à®±à®¤à¯',
 'à®ªà¯Šà®Ÿà¯à®Ÿà¯ˆ',
 'à®®à®¾à®™à¯à®•à®¾',
 'à®•à¯‹à®´à®¿',
 'à®®à¯à®²à¯ˆ',
 'à®ªà®±à®¤à¯à®¤à®¾à®¯à¯‹à®²à®¿',
 'à®“à®•à¯à®•',
 'à®¤à¯‡à®µà®Ÿà®¿à®¯à®¾_à®®à®µà®©à¯',
 'à®¤à¯‡à®µà®Ÿà®¿à®¯à®¾ à®ªà®šà®™à¯à®•à®³à®¾',
 'à®ªà¯à®£à¯à®Ÿà¯ˆ',
 'atrocity_act',
 'arrestlucknowgirl',
 'fakecasewalibiwi',
 'gasti',
 '#lovejihad',
 '#misandry',
 '#piddilover',
 '#pseudohindus',
 '#rheality_today',
 '#sanghans',
 '#saveoursons',
 '#scstact',
 'ðŸ‘',
 '#à¤œà¤¿à¤¹à¤¾à¤¦à¥€_à¤®à¥à¤•à¥à¤¤_à¤¨à¤µà¤°à¤¾à¤¤à¥à¤°à¤¿',
 '#à®“à®¤à¯à®¤',
 'à¤†à¤¤à¤‚à¤•à¤µà¤¾à¤¦à¥€',
 'à¤•à¤šà¤°à¤¾',
 'à¤•à¤‚à¤œà¤°',
 'à¤•à¤Ÿà¤¾ à¤²à¥à¤‚à¤¡',
 'à¤•à¤²à¤®à¥à¤‚à¤¹à¥€',
 'à¤•à¤¸à¤¾à¤ˆ',
 'à¤•à¤¾à¤®_à¤µà¤¾à¤²à¥€_à¤¬à¤¾à¤ˆ',
 'à¤•à¤¾à¤²à¤¾ à¤Ÿà¥‡à¤‚à¤Ÿ',
 'à¤•à¤¾à¤²à¥€ à¤•à¤²à¥‚à¤Ÿà¥€',
 'à¤•à¤¾à¤²à¥‡ à¤¤à¤µà¥‡',
 'à¤•à¤¿à¤¨à¥à¤¨à¤°',
 'à¤•à¥à¤²à¥à¤Ÿà¤¾',
 'à¥™à¥à¤¸à¤°à¤¾',
 'à¤—à¤Ÿà¤° à¤ªà¥ˆà¤¦à¤¾à¤‡à¤¶',
 'à¤—à¤£à¤¿à¤•à¤¾',
 'à¤—à¤¦à¥à¤¦à¤¾à¤°',
 'à¤—à¤§à¥€',
 'à¤˜à¤Ÿà¤¿à¤¯à¤¾ à¤”à¤°à¤¤',
 'à¤šà¤‚à¤¡à¤¾à¤²',
 'à¤šà¤‚à¤¡à¤¾à¤²_à¤šà¥Œà¤•à¤¡à¤¼à¥€',
 'à¤šà¤®à¤šà¤¾',
 'à¤šà¤®à¤¾à¤°',
 'à¤šà¤°à¤¿à¤¤à¥à¤°à¤¹à¥€à¤¨',
 'à¤šà¤¾à¤Ÿà¥à¤•à¤¾à¤°',
 'à¤šà¤¾à¤²à¥‚ à¤”à¤°à¤¤',
 'à¤šà¥à¤¦à¤¾à¤ˆ',
 'à¤šà¥à¤¸à¤²à¥à¤®à¤¾à¤¨',
 'à¤šà¥à¤¸à¥à¤²à¤¾à¤®à¤¿',
 'à¤šà¥à¤¸à¥à¤²à¤¿à¤®',
 'à¤šà¥‚à¤šà¥€',
 'à¤šà¥‚à¥œà¤¾',
 'à¤šà¥‚à¤¸',
 'à¤›à¤¿à¤¨à¤¾à¤²',
 'à¤œà¤‚à¤—à¤²à¥€',
 'à¤œà¤²à¥€',
 'à¤œà¤¾à¤¹à¤¿à¤²_à¤”à¤°à¤¤',
 'à¤œà¥‚à¤¤à¤¾ à¤–à¤¾à¤¯à¥‡à¤—à¥€',
 'à¤œà¥‹à¤°à¥‚ à¤•à¤¾ à¤—à¥à¤²à¤¾à¤®',
 'à¤à¥‚à¤ à¥€ à¤”à¤°à¤¤',
 'à¤›à¥à¤¤à¥€à¤¯à¥‡',
 'à¤¤à¤µà¤¾à¤‡à¥ž',
 'à¤¦à¤²à¤¾à¤²',
 'à¤¦à¥‡à¤¹à¤¾à¤¤à¤¨',
 'à¤¦à¥‡à¤¹à¤¾à¤¤à¥€ à¤”à¤°à¤¤',
 'à¤¦à¥‹ à¤•à¥Œà¥œà¥€ à¤•à¥€ à¤”à¤°à¤¤',
 'à¤¦à¥‹à¤—à¤²à¥€',
 'à¤§à¥‹à¤¬à¥€',
 'à¤¨à¤²à¥à¤²à¥€',
 'à¤¨à¤¾à¤œà¤¾à¤¯à¥›',
 'à¤¨à¤¾à¤²',
 'à¤ªà¤¨à¥Œà¤¤à¥€',
 'à¤ªà¤¿à¤›à¤µà¤¾à¥œà¤¾',
 'à¤ªà¥‡à¤²',
 'à¤ªà¥ˆà¤° à¤•à¥€ à¤œà¥‚à¤¤à¥€',
 'à¤ªà¥‰à¤Ÿà¥€',
 'à¤«à¥‡à¤®à¤¿à¤¨à¤¿à¤œà¤®',
 'à¤¬à¤¤à¥à¤¤à¤®à¥€à¥›',
 'à¤¬à¤¹à¤¨ à¤•à¥€ à¤²à¥‹à¥œà¥€',
 'à¤¬à¤¹à¤¨ à¤šà¥‹à¤¦',
 'à¤¬à¤¹à¤¨à¤œà¥€',
 'à¤¬à¤¾à¥›à¤¾à¤°à¥‚ à¤”à¤°à¤¤',
 'à¤¬à¥€à¤¬à¥€',
 'à¤¬à¥à¤°à¤–à¤¾ à¤§à¤¤à¥à¤¤',
 'à¤¬à¥à¤°à¤–à¥‡ à¤µà¤¾à¤²à¥€',
 'à¤¬à¥à¤²à¥à¤²à¥€',
 'à¤¬à¥‡à¤¹à¥‚à¤¦à¤¾ à¤”à¤°à¤¤',
 'à¤¬à¥ˆà¤² à¤¬à¥à¤¦à¥à¤§à¤¿',
 'à¤­à¤‚à¤—à¥€',
 'à¤­à¥œà¤µà¤¾',
 'à¤­à¤¦à¥à¤¦à¥€ à¤”à¤°à¤¤',
 'à¤­à¤¾à¤‚à¤¡',
 'à¤­à¤¾à¤‚à¤¡ à¤”à¤°à¤¤',
 'à¤­à¤¾à¥œà¥‡ à¤•à¤¾ à¤Ÿà¤Ÿà¥à¤Ÿà¥‚',
 'à¤­à¤¾à¤°à¤¤ à¤¤à¥‡à¤°à¥‡ à¤Ÿà¥à¤•à¤¡à¤¼à¥‡ à¤—à¥ˆà¤‚à¤—',
 'à¤­à¥‚à¤¤à¤¨à¥€ à¤•à¥‡',
 'à¤­à¥‹à¤‚à¤•à¤¨à¤¾',
 'à¤­à¥‹à¤¸à¤¡à¤¼à¥€wala',
 'à¤®à¤° à¤•à¥à¤¯à¥‹à¤‚ à¤¨à¤¹à¥€à¤‚ à¤œà¤¾à¤¤à¥€',
 'à¤®à¤¹à¤¾à¤°',
 'à¤®à¤¾ à¤•à¥‡ à¤­à¥‹à¤¸à¤¡à¤¼à¥‡',
 'à¤®à¤¾à¤¦à¤°à¤œà¤¾à¤¤',
 'à¤®à¥à¤²à¤¿à¤¯à¤¾',
 'à¤®à¥à¤à¤¹ à¤®à¥ˆà¤‚ à¤²à¥‡',
 'à¤®à¥‚à¤¤',
 'à¤®à¥‡à¤¡à¤® à¤œà¥€',
 'à¤®à¥‹à¤Ÿà¥€ à¤­à¥ˆà¤‚à¤¸',
 'à¤®à¥à¤²à¥‡à¤šà¥à¤›à¤¾',
 'à¤°à¤¾à¤‚à¤¡',
 'à¤²à¤¾à¤¨à¤¤à¥€',
 'à¤²à¥‡à¤¸à¥à¤¬à¤¿à¤¯à¤¨',
 'à¤²à¥‹à¥œà¥‚',
 'à¤²à¥Œà¥œà¤¾',
 'à¤²à¥Œà¥œà¥‡',
 'à¤µà¤¿à¤•à¥à¤Ÿà¤¿à¤® à¤•à¤¾à¤°à¥à¤¡',
 'à¤¶à¤°à¤¿à¤¯à¤¾',
 'à¤¸à¤¾à¤²à¥€',
 'à¤¸à¥à¤²à¥à¤²à¥€',
 'à¤¸à¥‚à¤…à¤°',
 'à¤¸à¥‚à¤…à¤° à¤•à¥€ à¤”à¤²à¤¾à¤¦',
 'à¤¸à¥Œ à¤Ÿà¤•à¤¾ à¤Ÿà¤‚à¤š à¤®à¤¾à¤²',
 'à¤¸à¥à¤•à¤°à¥à¤Ÿ à¤µà¤¾à¤²à¥€ à¤¬à¤¾à¤ˆ',
 'à¤¹à¤—à¤¨à¤¾',
 'à¤¹à¤¬à¤¶à¥€',
 'à¤¹à¤°à¤¾à¤®à¤œà¤¾à¤¦à¥€',
 'à¤¹à¤°à¤¾à¤®à¥€',
 'à¤¹à¤²à¤¾à¤²à¤¾',
 'à¤¹à¤¿à¤œà¤°à¤¾',
 'à¤¹à¤¿à¤œà¤¼à¤°à¤¾à¤ªà¤‚à¤¤à¥€',
 'à¤¹à¤¿à¤²à¤¾à¤“à¤—à¥€',
 'à¤®à¤¾à¤¦à¤°à¤šà¥‹à¤¦',
 'à¤­à¥‹à¤¸à¥à¤¡à¥€à¤•à¥‡',
 'à¤Ÿà¤Ÿà¥à¤Ÿà¥€',
 'à¤œà¤¾ à¤¨à¤¾à¤²à¥€ à¤¸à¤¾à¤«à¤¼ à¤•à¤°à¤•à¥‡ à¤†',
 'à¤†à¤‚à¤Ÿà¥€',
 'à®²à¯‚à®šà¯ à®•à¯‚',
 'à®ªà¯Šà®Ÿà¯à®Ÿà¯ˆ à®¨à®¾à®¯à¯',
 'à®²à¯‚à®šà¯',
 'à®¤à¯‡à®µà®¿à®Ÿà®¿à®¯à®¾à®³à¯à®•à¯à®•à¯ à®ªà¯Šà®±à®¨à¯à®¤à®µà®©à¯',
 'à®¤à¯‡à®µà®¿à®Ÿà®¿à®¯à®¾ à®ªà¯à®£à¯à®Ÿ',
 'à®šà¯‚à®¤à¯à®¤à®Ÿà®¿',
 'à®•à¯‚fire',
 '#à¤›à¤•à¥à¤•à¤¾',
 '#à®•à¯Šà®®à¯à®®à®¾',
 '#à®™à¯à®•à¯‹à®¤à¯à®¤à®¾',
 '#à®šà®¾à®®à®¾à®©à¯',
 '#à®ªà¯†à®°à®¿à®¯à®¾à®°à®¾à®µà®¤à¯_à®®à®¯à®¿à®°à®¾à®µà®¤à¯',
 '#à®ªà¯Šà®Ÿà¯à®Ÿà¯ˆ',
 '#à®®à¯à®Ÿà¯à®Ÿà®¾à®‰à®ªà®¿',
 'anti national tukde',
 '498a',
 'bimbo',
 'toxic aunty',
 'toilet saaf',
 'buzlim',
 'characterless woman',
 'chinky',
 'black pepper',
 'dull-it',
 'dynast sycophants',
 'ferrorists',
 'izzlam',
 'katwa',
 'muzlim',
 'naachne waali',
 'ola u uber',
 'pak agents',
 'pheminist',
 'pissfull',
 'rice bags',
 'scumbags',
 'secular',
 'sissy',
 'dented-painted',
 'toilet + saaf',
 'sunflowerà®£à¯à®Ÿà¯ˆ_emoji',
 'sunflowerà®£à¯à®Ÿà¯ˆ',
 'scumreds',
 'samlaingik',
 'ma ki chui',
 'jersey cow',
 'burnol',
 'victim card']

def regex_exact_slurs(tweet,slurs_list_lower):
      
    result = {}
    
    matches = re.findall(r"(?=("+'|'.join(slurs_list_lower)+r"))", tweet.lower())
    
    slurs = []
    tokens = []
    #print(matches)
    
    for match in matches:
    
        print(match)
        
        slurs.append(match)
        
        tokens.append(match)
        
        tweet = tweet.replace(match,'----')
    
        # token,slur key pair
        result.update({match:match})
    
    return tweet,result

def approx_matching_slurs(tweet,slurs_list_lower,threshold_score=70):
    
    print("\nApprox matching")
    check = 0
         
    match_dict = dict(process.extract(tweet,slurs_list_lower,limit = 10,scorer=fuzz.partial_ratio))

    matches = match_dict.keys()
    
    # to compare the distance of top 10 matching slurs to find the right matching
    dis_dict = {}
    token_slur_dict = {}
    
    
    for slur in matches:
        
        for token in tweet.split(' '):
            
            """
            Can add memoization here
            
            -Check if the distance b/w token and match is already calculated
            """
            
            if (token,slur) not in token_slur_dict:
                
                dis = lev.distance(token,slur)
                            
                token_slur_dict[(token,slur)] = dis
            
                if dis in dis_dict:
                    dis_dict[dis].append((token,slur))
            
                else:
                    dis_dict[dis] = [(token,slur)]
            
    
    dist_sort = dict(sorted(dis_dict.items()))
   
    result_dict = {}
    result = {}
    
    for dist,match in dist_sort.items():
        
        #print(dist,match)
        loop_break = 0
        
        for token,slur in match:
            
            #does it work for hin and tamil?
            if token:
                
                #print(slur)
                #print(match_dict[slur])
    
                if (slur[0].lower() == token[0].lower()) and (match_dict[slur] >= threshold_score) and (token.lower() not in ['muslim','muslims']):
                    
                    #print(f'slur,token : {slur} {token}')
                    result_dict[('slur','token')] = (slur,token)
                    
                    #token,slur value pair
                    result.update({token:slur})
                    
                  
                    tweet = tweet.replace(token,'----')
                    
                    # to iterate all the matches in that dict
                    loop_break = 1
                    
        if loop_break:
            break
                
    return tweet,result

def slur_replacement_slurs(tweet,slurs_list_lower=slurs_list_lower,threshold_score=90):
    
    tweet1,exact_result = regex_exact_slurs(tweet,slurs_list_lower)
    print(f'exact : {exact_result}')
    tweet2,approx_result = approx_matching_slurs(tweet1,slurs_list_lower,threshold_score=threshold_score)
    print(f'approx : {approx_result}')
    
    exact_result.update(approx_result)
    print(f'combined : {exact_result}')
    
    print(f'\n original tweet : {tweet}\n')
    print(f'final tweet : {tweet2}')
    
    return tweet2

# if __name__ == "__main__":

#     return slur_replacement_slurs(tweet,)
